# Hybrid RAG System - Retrieval-Augmented Generation

A comprehensive implementation of a Hybrid RAG system combining **Dense Vector Retrieval (FAISS)**, **Sparse Keyword Retrieval (BM25)**, and **Reciprocal Rank Fusion (RRF)** to answer questions from 500 Wikipedia articles.

## ğŸš€ Quick Start

**One-command setup (macOS/Linux):**
```bash
./run_all.sh
```

**One-command setup (Windows):**
```cmd
run_all.bat
```

This will automatically:
1. Set up virtual environment
2. Install all dependencies
3. Collect 500 Wikipedia articles
4. Build FAISS and BM25 indexes
5. Generate 100 evaluation questions
6. Run comprehensive evaluation
7. Optionally launch the UI

## ğŸ¯ Project Overview

This project implements a state-of-the-art Hybrid RAG system that:
- Combines dense and sparse retrieval for superior performance
- Uses Reciprocal Rank Fusion to intelligently merge results
- Generates answers using Flan-T5 language model
- Includes comprehensive evaluation with 100 generated questions
- Features innovative evaluation techniques (ablation studies, error analysis, LLM-as-judge)

## ğŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  User Query                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dense Retrievalâ”‚   â”‚Sparse Retrievalâ”‚
â”‚  (FAISS + SE)  â”‚   â”‚     (BM25)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Reciprocal Rank     â”‚
        â”‚ Fusion (RRF)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Top-N Chunks       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  LLM Generation     â”‚
        â”‚   (Flan-T5)         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Generated Answer  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—‚ï¸ Project Structure

```
ConvAI_assingment_2/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ fixed_urls.json          # 200 fixed Wikipedia URLs
â”‚   â”œâ”€â”€ corpus.json              # Processed corpus with chunks
â”‚   â””â”€â”€ questions_100.json       # 100 evaluation questions
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection.py       # Wikipedia data collection
â”‚   â”œâ”€â”€ rag_system.py            # Main RAG implementation
â”‚   â””â”€â”€ question_generation.py   # Question generation
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py               # Evaluation metrics (MRR, NDCG, BERTScore)
â”‚   â”œâ”€â”€ innovative_eval.py       # Advanced evaluation features
â”‚   â””â”€â”€ pipeline.py              # Automated evaluation pipeline
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ faiss_index             # Dense vector index
â”‚   â””â”€â”€ bm25_index.pkl          # Sparse BM25 index
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ evaluation_results.json  # Detailed results
â”‚   â”œâ”€â”€ evaluation_results.csv   # Tabular results
â”‚   â”œâ”€â”€ visualizations/          # Charts and plots
â”‚   â”œâ”€â”€ ablation/                # Ablation study results
â”‚   â””â”€â”€ errors/                  # Error analysis
â”œâ”€â”€ app.py                       # Streamlit UI
â”œâ”€â”€ config.yaml                  # Configuration
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- 8GB+ RAM (16GB recommended)
- GPU optional (recommended for faster processing)

### Setup Instructions

#### Option 1: Automated Setup (Recommended)

**macOS/Linux:**
```bash
./run_all.sh
```

**Windows:**
```cmd
run_all.bat
```

This single command will:
- Create virtual environment
- Install all dependencies
- Collect 500 Wikipedia articles
- Build FAISS and BM25 indexes
- Generate 100 evaluation questions
- Run complete evaluation
- Optionally launch the Streamlit UI

**Total time**: ~90-150 minutes (mostly automated)

#### Option 2: Manual Setup

1. **Clone/Download the repository**
```bash
cd ConvAI_assingment_2
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Download NLTK data**
```python
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

#### Option 3: Interactive Setup

For guided step-by-step setup:
```bash
python setup.py
```

## ğŸ“š Usage Guide

### Quick Test

Run a quick test without full evaluation:
```bash
python quick_test.py
```

This will run 5 sample queries and show results.

### Step 1: Data Collection

Collect 200 fixed + 300 random Wikipedia URLs and process them into chunks:

```bash
# Generate fixed URLs first
python generate_fixed_urls.py

# Then collect all data
python src/data_collection.py
```

This creates:
- `data/fixed_urls.json` - 200 fixed URLs (remains constant)
- `data/corpus.json` - Processed corpus with all chunks

**Time**: ~30-60 minutes depending on internet speed

### Step 2: Build Indexes

Build dense (FAISS) and sparse (BM25) indexes:

```bash
python -c "
from src.rag_system import HybridRAGSystem
rag = HybridRAGSystem()
rag.load_corpus()
rag.build_dense_index()
rag.build_sparse_index()
"
```

This creates:
- `models/faiss_index` - Dense vector index
- `models/bm25_index.pkl` - Sparse keyword index

**Time**: ~10-20 minutes

### Step 3: Generate Evaluation Questions

Generate 100 diverse questions for evaluation:

```bash
python src/question_generation.py
```

This creates:
- `data/questions_100.json` - 100 Q&A pairs with metadata

**Time**: ~5-10 minutes

### Step 4: Run Evaluation Pipeline

Run the complete automated evaluation:

```bash
python evaluation/pipeline.py
```

This performs:
1. Evaluates all 100 questions
2. Calculates MRR, NDCG@5, BERTScore
3. Runs ablation study (dense vs sparse vs hybrid)
4. Performs error analysis
5. Generates visualizations
6. Saves comprehensive reports

**Output**: All results in `reports/` directory

**Time**: ~30-60 minutes

### Step 5: Launch Streamlit UI

Start the interactive web interface:

```bash
streamlit run app.py
```

Access at: http://localhost:8501

## ğŸ“Š Evaluation Metrics

### 1. Mean Reciprocal Rank (MRR) - **MANDATORY**

**Purpose**: Measures how quickly the system identifies the correct source document.

**Calculation**:
```
For each query:
  RR = 1/rank (if found), 0 (if not found)
MRR = Average of all RR scores
```

**Interpretation**:
- 1.0: Perfect - correct URL always ranked first
- 0.7-1.0: Excellent
- 0.5-0.7: Good
- < 0.5: Needs improvement

### 2. BERTScore F1 - **CUSTOM METRIC 1**

**Why Chosen**: Evaluates semantic similarity using contextual embeddings, capturing meaning beyond lexical matching.

**Calculation**:
1. Compute BERT embeddings for tokens
2. Calculate cosine similarity matrix
3. Greedy matching for optimal alignment
4. F1 = 2 * (Precision * Recall) / (Precision + Recall)

**Interpretation**:
- > 0.9: Excellent semantic match
- 0.8-0.9: Good match
- 0.7-0.8: Moderate match
- < 0.7: Poor match

### 3. NDCG@5 - **CUSTOM METRIC 2**

**Why Chosen**: Evaluates ranking quality considering both relevance and position. Critical for RAG as position affects context quality.

**Calculation**:
```
DCG@5 = Î£(i=1 to 5) [rel_i / log2(i+1)]
NDCG@5 = DCG@5 / IDCG@5
```

**Interpretation**:
- 1.0: Perfect ranking
- 0.8-1.0: Excellent
- 0.6-0.8: Good
- < 0.6: Needs improvement

## ğŸ¨ Innovative Evaluation Features

1. **Ablation Study**: Compares dense-only, sparse-only, and hybrid performance
2. **Error Analysis**: Categorizes failures by type and question category
3. **LLM-as-Judge**: Uses LLM to evaluate answer quality
4. **Adversarial Testing**: Tests with negated and paraphrased questions
5. **Confidence Calibration**: Analyzes correlation between confidence and correctness
6. **Interactive Dashboard**: Real-time visualizations of all metrics

## ğŸ“ˆ Expected Results

Based on typical performance:

| Metric | Dense Only | Sparse Only | Hybrid (RRF) |
|--------|-----------|-------------|--------------|
| MRR | 0.45-0.60 | 0.40-0.55 | **0.55-0.70** |
| NDCG@5 | 0.50-0.65 | 0.45-0.60 | **0.60-0.75** |
| BERTScore F1 | 0.65-0.75 | 0.60-0.70 | **0.70-0.80** |

Hybrid approach typically outperforms individual methods by 10-20%.

## ğŸ¯ Key Features

### Data Collection
- âœ… 200 fixed Wikipedia URLs (diverse topics)
- âœ… 300 random URLs per run
- âœ… Intelligent chunking (200-400 tokens, 50-token overlap)
- âœ… Metadata tracking (URL, title, chunk IDs)

### Retrieval System
- âœ… Dense retrieval with sentence-transformers
- âœ… Sparse retrieval with BM25
- âœ… Reciprocal Rank Fusion (k=60)
- âœ… Configurable top-K and top-N

### Generation
- âœ… Flan-T5-base for answer generation
- âœ… Context-aware prompting
- âœ… Configurable generation parameters

### Evaluation
- âœ… 100 diverse questions (factual, comparative, inferential, multi-hop)
- âœ… 3 comprehensive metrics (MRR, BERTScore, NDCG)
- âœ… Ablation studies
- âœ… Error analysis with categorization
- âœ… Rich visualizations

### User Interface
- âœ… Interactive Streamlit app
- âœ… Real-time query processing
- âœ… Source visualization with scores
- âœ… Performance metrics display
- âœ… Response time tracking

## ğŸ”§ Configuration

Edit `config.yaml` to customize:

```yaml
models:
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  generation_model: "google/flan-t5-base"

retrieval:
  dense:
    top_k: 20
  sparse:
    top_k: 20
  rrf:
    k: 60
    final_top_n: 5
```

## ğŸ“ Fixed URLs

The 200 fixed Wikipedia URLs cover diverse topics:

- **Science**: Physics, Chemistry, Biology, Astronomy, Geology
- **Technology**: AI, Computer Science, Robotics, Internet, Quantum Computing
- **History**: Ancient Egypt, Roman Empire, WWII, Renaissance
- **Geography**: Mountains, Rivers, Oceans, Countries
- **Arts**: Famous artists, Classical music, Literature
- **Sports**: Olympic Games, FIFA World Cup, Cricket
- **Philosophy**: Major philosophers, Ethics, Metaphysics
- **Mathematics**: Calculus, Linear Algebra, Statistics
- **Medicine**: Anatomy, Genetics, Immunology

Full list in `data/fixed_urls.json`

## ğŸ› Troubleshooting

### Issue: Out of Memory
**Solution**: Reduce batch size in `config.yaml` or use smaller model

### Issue: Slow Indexing
**Solution**: Use GPU if available, or reduce corpus size for testing

### Issue: Low Scores
**Solution**: 
- Check if questions match corpus topics
- Adjust RRF k parameter
- Try different embedding models

### Issue: Import Errors
**Solution**: 
```bash
pip install --upgrade -r requirements.txt
```

## ğŸ“Š Sample Output

```
OVERALL RESULTS:
  MRR:           0.6234
  NDCG@5:        0.6891
  BERTScore F1:  0.7456
  Precision@5:   0.4200
  Recall@5:      0.5834
  ROUGE-L:       0.3987

Performance by Question Type:
  factual: MRR=0.72, NDCG=0.75
  comparative: MRR=0.58, NDCG=0.65
  inferential: MRR=0.51, NDCG=0.61
  multi_hop: MRR=0.49, NDCG=0.58
```

## ğŸ“ Academic Context

This project is designed for educational purposes as part of a Conversational AI assignment. It demonstrates:
- Modern RAG architecture
- Hybrid retrieval techniques
- Comprehensive evaluation methodologies
- Best practices in ML system development

## ğŸ¤ Contributing

This is an educational project. Feel free to:
- Experiment with different models
- Add new evaluation metrics
- Improve the UI
- Optimize performance

## ğŸ“„ License

This project is for educational purposes only.

## ğŸ™ Acknowledgments

- **Sentence Transformers**: For embedding models
- **FAISS**: For efficient vector search
- **Rank-BM25**: For keyword retrieval
- **Hugging Face**: For LLM models
- **Streamlit**: For the UI framework

---

## ğŸš€ Quick Start Commands

```bash
# Full pipeline (run in order)
python src/data_collection.py
python -c "from src.rag_system import HybridRAGSystem; rag = HybridRAGSystem(); rag.load_corpus(); rag.build_dense_index(); rag.build_sparse_index()"
python src/question_generation.py
python evaluation/pipeline.py
streamlit run app.py
```

**Total setup time**: ~1-2 hours
**Total execution time**: ~2-3 hours

---

**Built with â¤ï¸ for Conversational AI**
