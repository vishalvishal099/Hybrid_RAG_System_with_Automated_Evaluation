# Hybrid RAG System with Automated Evaluation

## ğŸ“‹ Submission Package

**GitHub Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)

**Course:** BITS Pilani - Conversational AI Assignment 2

**Date:** February 7, 2026

---

## ğŸ“ Folder Structure

```
Hybrid_RAG_System_with_Automated_Evaluation/
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ chromadb_rag_system.py     # Main RAG system implementation
â”‚   â”œâ”€â”€ app_chromadb.py            # Streamlit UI
â”‚   â”œâ”€â”€ evaluate_chromadb_fast.py  # Evaluation pipeline
â”‚   â””â”€â”€ generate_report.py         # Report generation
â”‚
â”œâ”€â”€ data/                          # Data files
â”‚   â”œâ”€â”€ fixed_urls.json            # 200 fixed Wikipedia URLs
â”‚   â”œâ”€â”€ corpus.json                # Preprocessed corpus (14.5MB)
â”‚   â”œâ”€â”€ questions_100.json         # 100 evaluation questions
â”‚   â””â”€â”€ indexes/                   # BM25 index files
â”‚
â”œâ”€â”€ chroma_db/                     # Vector database (212MB)
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ METRIC_JUSTIFICATION.md    # Metric selection rationale
â”‚   â”œâ”€â”€ ERROR_ANALYSIS.md          # Failure analysis
â”‚   â”œâ”€â”€ EVALUATION_REPORT.md       # Full evaluation report
â”‚   â”œâ”€â”€ architecture_diagram.png   # System architecture
â”‚   â””â”€â”€ *.png                      # Visualizations
â”‚
â”œâ”€â”€ reports/                       # Generated reports
â”‚   â””â”€â”€ Hybrid_RAG_Evaluation_Report.pdf
â”‚
â”œâ”€â”€ screenshots/                   # UI screenshots
â”‚   â”œâ”€â”€ 01_query_interface.png
â”‚   â”œâ”€â”€ 02_method_comparison.png
â”‚   â””â”€â”€ 03_evaluation_results.png
â”‚
â”œâ”€â”€ evaluation/                    # Evaluation results
â”‚   â”œâ”€â”€ evaluation_results_chromadb.csv
â”‚   â”œâ”€â”€ evaluation_summary_chromadb.json
â”‚   â””â”€â”€ evaluation_report_chromadb.html
â”‚
â”œâ”€â”€ README.md                      # Main documentation
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ start_ui.sh                    # Quick start script
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- 4GB+ RAM (for embeddings and LLM)

### Installation

```bash
# Clone repository
git clone https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation.git
cd Hybrid_RAG_System_with_Automated_Evaluation

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Run the Application

```bash
# Start Streamlit UI
./start_ui.sh

# Or manually:
streamlit run app_chromadb.py
```

### Run Evaluation

```bash
# Full evaluation (100 questions Ã— 3 methods)
python evaluate_chromadb_fast.py

# Generate reports
python generate_report.py
```

---

## ğŸ—ï¸ System Architecture

### Components

| Component | Technology | Description |
|-----------|------------|-------------|
| Dense Retrieval | ChromaDB + all-MiniLM-L6-v2 | 384-dim vector embeddings |
| Sparse Retrieval | BM25 + NLTK | Keyword-based matching |
| Fusion | RRF (k=60) | Reciprocal Rank Fusion |
| Generation | FLAN-T5-base | Text-to-text transformer |
| Interface | Streamlit | Interactive web UI |

### Data Flow

1. **Query Input** â†’ User enters question via Streamlit
2. **Dense Search** â†’ Embed query, search ChromaDB
3. **Sparse Search** â†’ BM25 keyword matching
4. **RRF Fusion** â†’ Combine rankings with k=60
5. **Generation** â†’ FLAN-T5 generates answer from context
6. **Display** â†’ Show answer, sources, and metrics

---

## ğŸ“Š Evaluation Results

### Performance Summary

| Method | MRR | Recall@10 | Avg Time |
|--------|-----|-----------|----------|
| Dense | 0.3025 | 0.33 | 5.86s |
| **Sparse (BM25)** | **0.4392** | **0.47** | 5.53s |
| Hybrid (RRF) | 0.3783 | 0.43 | 6.37s |

**Key Finding:** BM25 outperforms Dense by 45% on MRR for Wikipedia-based QA.

### Evaluation Dataset

- **100 Questions** across 4 types:
  - Factual (59)
  - Comparative (15)
  - Inferential (11)
  - Multi-hop (15)

---

## ğŸ“š Documentation

| Document | Description | Link |
|----------|-------------|------|
| Metric Justification | Why MRR, Recall@10, Answer F1 | [docs/METRIC_JUSTIFICATION.md](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/docs/METRIC_JUSTIFICATION.md) |
| Error Analysis | Failure categorization | [docs/ERROR_ANALYSIS.md](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/docs/ERROR_ANALYSIS.md) |
| Full Report | Comprehensive evaluation | [docs/EVALUATION_REPORT.md](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/docs/EVALUATION_REPORT.md) |
| PDF Report | Printable report | [reports/Hybrid_RAG_Evaluation_Report.pdf](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/reports/Hybrid_RAG_Evaluation_Report.pdf) |

---

## ğŸ“¸ Screenshots

### Query Interface
![Query Interface](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/screenshots/01_query_interface.png)

### Method Comparison
![Method Comparison](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/screenshots/02_method_comparison.png)

### Evaluation Results
![Evaluation Results](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/screenshots/03_evaluation_results.png)

---

## ğŸ”— Key Files

### Source Code
- [chromadb_rag_system.py](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/chromadb_rag_system.py) - Core RAG implementation
- [app_chromadb.py](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/app_chromadb.py) - Streamlit UI
- [evaluate_chromadb_fast.py](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/evaluate_chromadb_fast.py) - Evaluation pipeline

### Data Files
- [data/questions_100.json](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/data/questions_100.json) - 100 evaluation questions
- [data/fixed_urls.json](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/data/fixed_urls.json) - 200 fixed Wikipedia URLs

### Results
- [evaluation_results_chromadb.csv](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/evaluation_results_chromadb.csv) - 300 evaluation rows
- [evaluation_summary_chromadb.json](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/evaluation_summary_chromadb.json) - Summary metrics

---

## ğŸ“‹ Requirements Checklist

### Section 1: Hybrid RAG System (10 pts) âœ…
- [x] Dense Vector Retrieval (ChromaDB)
- [x] Sparse Keyword Retrieval (BM25)
- [x] RRF Fusion (k=60)
- [x] Response Generation (FLAN-T5)
- [x] Interactive UI (Streamlit)

### Section 2: Evaluation Framework (10 pts)
- [x] 100 Q&A pairs generated
- [x] MRR metric implemented
- [x] Recall@10 metric implemented
- [x] Answer F1 metric implemented
- [x] Automated evaluation pipeline
- [x] HTML/CSV/JSON reports

### Submission Requirements
- [x] Python source code
- [x] PDF evaluation report
- [x] Screenshots (3+)
- [x] README documentation
- [x] 100-question dataset
- [x] Evaluation results

---

## ğŸ“„ License

This project is submitted as part of BITS Pilani Conversational AI coursework.

---

**Generated:** February 7, 2026
