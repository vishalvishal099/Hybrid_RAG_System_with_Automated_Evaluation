# Hybrid RAG System with Automated Evaluation# Hybrid RAG System with Automated Evaluation



[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue)](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue)](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)



A comprehensive implementation of a **Hybrid RAG System** combining **Dense Vector Retrieval (ChromaDB)**, **Sparse Keyword Retrieval (BM25)**, and **Reciprocal Rank Fusion (RRF)** to answer questions from Wikipedia articles.A comprehensive implementation of a **Hybrid RAG System** combining **Dense Vector Retrieval (ChromaDB)**, **Sparse Keyword Retrieval (BM25)**, and **Reciprocal Rank Fusion (RRF)** to answer questions from Wikipedia articles.



**GitHub Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)**GitHub Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)



**Last Updated:** February 8, 2026---



---## ğŸš€ Quick Start



## ğŸš€ Quick Start### Prerequisites

- Python 3.10+

### Prerequisites- 4GB+ RAM

- Python 3.10+

- 4GB+ RAM### Installation



### Installation```bash

# Clone repository

```bashgit clone https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation.git

# Clone repositorycd Hybrid_RAG_System_with_Automated_Evaluation

git clone https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation.git

cd Hybrid_RAG_System_with_Automated_Evaluation# Create virtual environment

python -m venv venv

# Create virtual environmentsource venv/bin/activate  # Windows: venv\Scripts\activate

python -m venv venv

source venv/bin/activate  # Windows: venv\Scripts\activate# Install dependencies

pip install -r requirements.txt

# Install dependencies```

pip install -r requirements.txt

```### Run the Application



### Run the Application```bash

# Start Streamlit UI

```bash./start_ui.sh

# Start Streamlit UI

streamlit run app_chromadb.py --server.port 8502# Or manually:

```streamlit run app_chromadb.py

```

**Access URL:** http://localhost:8502

### Run Evaluation

### Run Evaluation

```bash

```bash# Full evaluation (100 questions Ã— 3 methods)

# Full evaluation (100 questions Ã— 3 methods)python evaluate_chromadb_fast.py

python evaluate_chromadb_fast.py

```# Generate reports

python generate_report.py

---```



## ğŸ¯ Project Overview---



This project implements a state-of-the-art Hybrid RAG system that:## ğŸ¯ Project Overview

- Combines **dense** (ChromaDB + MiniLM) and **sparse** (BM25) retrieval

- Uses **Reciprocal Rank Fusion (RRF)** with k=60 to merge resultsThis project implements a state-of-the-art Hybrid RAG system that:

- Generates answers using **FLAN-T5** language model (248M parameters)- Combines **dense** (ChromaDB + MiniLM) and **sparse** (BM25) retrieval

- Includes comprehensive evaluation with **100 generated questions**- Uses **Reciprocal Rank Fusion (RRF)** with k=60 to merge results

- Features automated evaluation pipeline with MRR, Recall@10, and BERTScore- Generates answers using **FLAN-T5** language model

- Interactive dashboard with **Dense/Sparse/Hybrid chunk comparison**- Includes comprehensive evaluation with **100 generated questions**

- Features automated evaluation pipeline with MRR, Recall@10, and Answer F1

### Key Statistics

---

| Metric | Value |

|--------|-------|## ğŸ“Š System Architecture

| Total URLs | 500 (200 fixed + 300 random) |

| Total Chunks | 7,519 segments |```

| Embedding Dimensions | 384 |â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

| Chunk Size | 500 characters |â”‚                    User Query                            â”‚

| Evaluation Questions | 100 |â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                       â”‚

---            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

            â”‚                     â”‚

## ğŸ“Š System Architecture    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”

    â”‚ Dense Retrievalâ”‚   â”‚Sparse Retrievalâ”‚

```    â”‚ (ChromaDB +    â”‚   â”‚    (BM25 +     â”‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  MiniLM-L6-v2) â”‚   â”‚     NLTK)      â”‚

â”‚                    User Query                            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚                     â”‚

                       â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚

            â”‚                     â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”            â”‚ Reciprocal Rank     â”‚

    â”‚ Dense Retrievalâ”‚   â”‚Sparse Retrievalâ”‚            â”‚ Fusion (k=60)       â”‚

    â”‚ (ChromaDB +    â”‚   â”‚    (BM25 +     â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”‚  MiniLM-L6-v2) â”‚   â”‚     NLTK)      â”‚                       â”‚

    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

            â”‚                     â”‚            â”‚   Top-K Chunks      â”‚

            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                       â”‚                       â”‚

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

            â”‚ Reciprocal Rank     â”‚            â”‚  Answer Generation  â”‚

            â”‚ Fusion (k=60)       â”‚            â”‚  (FLAN-T5-base)     â”‚

            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                       â”‚                       â”‚

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

            â”‚   Top-K Chunks      â”‚            â”‚   Generated Answer  â”‚

            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   + Source URLs     â”‚

                       â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”```

            â”‚  Answer Generation  â”‚

            â”‚  (FLAN-T5-base)     â”‚---

            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                       â”‚## ğŸ—‚ï¸ Project Structure

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

            â”‚   Generated Answer  â”‚```

            â”‚   + Source URLs     â”‚Hybrid_RAG_System_with_Automated_Evaluation/

            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚

```â”œâ”€â”€ chromadb_rag_system.py      # Core RAG implementation

â”œâ”€â”€ app_chromadb.py             # Streamlit UI (244 lines)

---â”œâ”€â”€ evaluate_chromadb_fast.py   # Evaluation pipeline

â”œâ”€â”€ generate_report.py          # Report generator

## ğŸ—‚ï¸ Project Structureâ”œâ”€â”€ start_ui.sh                 # Quick start script

â”‚

```â”œâ”€â”€ data/

Hybrid_RAG_System_with_Automated_Evaluation/â”‚   â”œâ”€â”€ fixed_urls.json         # 200 fixed Wikipedia URLs

â”‚â”‚   â”œâ”€â”€ corpus.json             # Preprocessed corpus (14.5MB)

â”œâ”€â”€ chromadb_rag_system.py      # Core RAG implementationâ”‚   â”œâ”€â”€ questions_100.json      # 100 evaluation questions

â”œâ”€â”€ app_chromadb.py             # Streamlit UI with chunk visualizationâ”‚   â””â”€â”€ indexes/                # BM25 index files

â”œâ”€â”€ evaluate_chromadb_fast.py   # Evaluation pipelineâ”‚

â”œâ”€â”€ error_analysis.py           # Error analysis moduleâ”œâ”€â”€ chroma_db/                  # ChromaDB vector database (212MB)

â”œâ”€â”€ api_chromadb.py             # REST API interfaceâ”‚

â”œâ”€â”€ build_chromadb_system.py    # System builderâ”œâ”€â”€ docs/

â”œâ”€â”€ setup.py                    # Package setupâ”‚   â”œâ”€â”€ METRIC_JUSTIFICATION.md # Metric selection rationale

â”œâ”€â”€ config.yaml                 # Configurationâ”‚   â”œâ”€â”€ ERROR_ANALYSIS.md       # Failure analysis

â”œâ”€â”€ requirements.txt            # Dependenciesâ”‚   â”œâ”€â”€ EVALUATION_REPORT.md    # Full evaluation report

â”œâ”€â”€ README.md                   # This fileâ”‚   â”œâ”€â”€ architecture_diagram.png

â”œâ”€â”€ SUBMISSION_REFERENCE.md     # Complete submission guideâ”‚   â””â”€â”€ *.png                   # Visualizations

â”‚â”‚

â”œâ”€â”€ data/â”œâ”€â”€ reports/

â”‚   â”œâ”€â”€ fixed_urls.json         # 200 fixed Wikipedia URLsâ”‚   â””â”€â”€ Hybrid_RAG_Evaluation_Report.pdf

â”‚   â”œâ”€â”€ corpus.json             # Preprocessed corpus (7,519 chunks)â”‚

â”‚   â”œâ”€â”€ questions_100.json      # 100 evaluation questionsâ”œâ”€â”€ screenshots/

â”‚   â””â”€â”€ adversarial_questions.json  # 30 adversarial questionsâ”‚   â”œâ”€â”€ 01_query_interface.png

â”‚â”‚   â”œâ”€â”€ 02_method_comparison.png

â”œâ”€â”€ chroma_db/                  # ChromaDB vector databaseâ”‚   â””â”€â”€ 03_evaluation_results.png

â”‚   â”œâ”€â”€ bm25_index.pkl          # BM25 indexâ”‚

â”‚   â”œâ”€â”€ bm25_corpus.pkl         # BM25 corpusâ”œâ”€â”€ evaluation_results_chromadb.csv     # 300 evaluation rows

â”‚   â””â”€â”€ stats.json              # Database statisticsâ”œâ”€â”€ evaluation_summary_chromadb.json    # Summary metrics

â”‚â”œâ”€â”€ evaluation_report_chromadb.html     # HTML report

â”œâ”€â”€ src/                        # Source modulesâ”‚

â”‚   â”œâ”€â”€ data_collection.py      # Wikipedia data collectorâ””â”€â”€ README.md                   # This file

â”‚   â”œâ”€â”€ semantic_chunker.py     # Semantic chunking```

â”‚   â”œâ”€â”€ rrf_fusion.py           # RRF implementation

â”‚   â”œâ”€â”€ rag_system.py           # RAG system---

â”‚   â””â”€â”€ indexing.py             # Indexing utilities

â”‚## ğŸ“ˆ Evaluation Results

â”œâ”€â”€ evaluation/                 # Evaluation framework

â”‚   â”œâ”€â”€ metrics.py              # Core metrics (MRR, BERTScore)### Performance Summary

â”‚   â”œâ”€â”€ novel_metrics.py        # Novel evaluation metrics

â”‚   â”œâ”€â”€ innovative_eval.py      # Innovative techniques| Method | MRR | Recall@10 | Avg Time (s) | Questions |

â”‚   â”œâ”€â”€ pipeline.py             # Evaluation pipeline|--------|-----|-----------|--------------|-----------|

â”‚   â””â”€â”€ comprehensive_metrics.py # Comprehensive metrics| Dense (ChromaDB) | 0.3025 | 0.33 | 5.86 | 100 |

â”‚| **Sparse (BM25)** | **0.4392** | **0.47** | 5.53 | 100 |

â”œâ”€â”€ docs/                       # Documentation| Hybrid (RRF) | 0.3783 | 0.43 | 6.37 | 100 |

â”‚   â”œâ”€â”€ METRIC_JUSTIFICATION.md # Metric selection rationale

â”‚   â”œâ”€â”€ NEW_FEATURES.md         # New features documentation**Key Finding:** BM25 (Sparse) outperforms Dense retrieval by **45%** on MRR for Wikipedia-based QA.

â”‚   â”œâ”€â”€ architecture_diagram.png

â”‚   â”œâ”€â”€ data_flow_diagram.png### Question Distribution

â”‚   â””â”€â”€ retrieval_heatmap.png

â”‚| Type | Count | Description |

â”œâ”€â”€ screenshots/                # UI screenshots|------|-------|-------------|

â”‚   â”œâ”€â”€ 01_query_interface.png| Factual | 59 | Direct fact-based questions |

â”‚   â”œâ”€â”€ 02_method_comparison.png| Comparative | 15 | Questions comparing concepts |

â”‚   â””â”€â”€ 03_evaluation_results.png| Inferential | 11 | Reasoning-based questions |

â”‚| Multi-hop | 15 | Questions requiring multiple sources |

â”œâ”€â”€ submission/                 # Submission package| **Total** | **100** | - |

â”‚   â”œâ”€â”€ 01_source_code/         # All source files

â”‚   â”œâ”€â”€ 02_data/                # Data files---

â”‚   â”œâ”€â”€ 03_vector_database/     # Database info

â”‚   â”œâ”€â”€ 04_evaluation_results/  # Results## ğŸ“š Documentation

â”‚   â”œâ”€â”€ 05_reports/             # PDF reports

â”‚   â”œâ”€â”€ 06_documentation/       # Docs| Document | Description | Link |

â”‚   â”œâ”€â”€ 07_visualizations/      # Charts|----------|-------------|------|

â”‚   â””â”€â”€ 08_screenshots/         # Screenshots| Metric Justification | Why MRR, Recall@10, Answer F1 | [docs/METRIC_JUSTIFICATION.md](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/docs/METRIC_JUSTIFICATION.md) |

â”‚| Error Analysis | Failure categorization | [docs/ERROR_ANALYSIS.md](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/docs/ERROR_ANALYSIS.md) |

â”œâ”€â”€ evaluation_results_chromadb.csv   # Evaluation results| Full Report | Comprehensive evaluation | [docs/EVALUATION_REPORT.md](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/docs/EVALUATION_REPORT.md) |

â””â”€â”€ evaluation_summary_chromadb.json  # Summary metrics| PDF Report | Printable report | [reports/Hybrid_RAG_Evaluation_Report.pdf](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/reports/Hybrid_RAG_Evaluation_Report.pdf) |

```

---

---

## ğŸ”— Key Source Files

## ğŸ“ˆ Evaluation Results

| File | Purpose | Link |

### Performance Summary|------|---------|------|

| `chromadb_rag_system.py` | Core RAG implementation | [View](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/chromadb_rag_system.py) |

| Method | MRR | Recall@10 | Avg Time (s) || `app_chromadb.py` | Streamlit UI | [View](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/app_chromadb.py) |

|--------|-----|-----------|--------------|| `evaluate_chromadb_fast.py` | Evaluation pipeline | [View](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/evaluate_chromadb_fast.py) |

| Dense (ChromaDB) | 0.3025 | 0.33 | 5.86 || `generate_report.py` | Report generation | [View](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/generate_report.py) |

| **Sparse (BM25)** | **0.4392** | **0.47** | 5.53 |

| Hybrid (RRF) | 0.3783 | 0.43 | 6.37 |---



**Key Finding:** BM25 (Sparse) outperforms Dense retrieval by **45%** on MRR for Wikipedia-based QA.## ğŸ“¸ Screenshots



### Generation Metrics### Query Interface

![Query Interface](screenshots/01_query_interface.png)

| Method | BLEU | ROUGE-L | BERTScore |

|--------|------|---------|-----------|### Method Comparison  

| Dense | 0.015 | 0.120 | 0.780 |![Method Comparison](screenshots/02_method_comparison.png)

| Sparse | 0.022 | 0.145 | 0.820 |

| Hybrid | 0.018 | 0.135 | 0.810 |### Evaluation Results

![Evaluation Results](screenshots/03_evaluation_results.png)

---

---

## ğŸ–¥ï¸ Interactive Dashboard Features

## ğŸ› ï¸ Technical Details

The Streamlit UI includes:

### Components

| Feature | Description |

|---------|-------------|| Component | Technology | Details |

| **Query Input** | Text area for entering questions ||-----------|------------|---------|

| **Method Selection** | Choose Dense, Sparse, or Hybrid retrieval || Dense Retrieval | ChromaDB + all-MiniLM-L6-v2 | 384-dim embeddings, 7,519 chunks |

| **Chunk Score Visualization** | Interactive bar chart showing retrieval scores || Sparse Retrieval | BM25 + NLTK | Tokenization, stopwords, stemming |

| **Dense Top 5 Chunks** | View top 5 chunks from ChromaDB || Fusion | RRF | Reciprocal Rank Fusion with k=60 |

| **Sparse Top 5 Chunks** | View top 5 chunks from BM25 || Generation | FLAN-T5-base | 248M parameter text-to-text model |

| **Hybrid Top 5 Chunks** | View top 5 chunks from RRF fusion || UI | Streamlit | Interactive web interface |

| **Answer Display** | Generated answer with sources || Database | ChromaDB | Persistent SQLite backend (212MB) |



---### Metrics



## ğŸ› ï¸ Technical Details| Metric | Formula | Purpose |

|--------|---------|---------|

### Components| **MRR** | (1/Q) Ã— Î£(1/rank_i) | Measures retrieval quality |

| **Recall@10** | \|Relevant âˆ© Retrieved@10\| / \|Relevant\| | Coverage in top 10 |

| Component | Technology | Details || **Answer F1** | 2Ã—(PÃ—R)/(P+R) | Token overlap with ground truth |

|-----------|------------|---------|

| Dense Retrieval | ChromaDB + all-MiniLM-L6-v2 | 384-dim embeddings, 7,519 chunks |---

| Sparse Retrieval | BM25 + NLTK | Tokenization with rank_bm25 |

| Fusion | RRF | Reciprocal Rank Fusion with k=60 |## ğŸ“‹ Requirements Checklist

| Generation | FLAN-T5-base | 248M parameter model |

| UI | Streamlit | Interactive web interface |### âœ… Section 1: Hybrid RAG System (10 pts)

- [x] Dense Vector Retrieval (ChromaDB + MiniLM)

### Metrics- [x] Sparse Keyword Retrieval (BM25)

- [x] RRF Fusion (k=60)

| Metric | Formula | Purpose |- [x] Response Generation (FLAN-T5)

|--------|---------|---------|- [x] Interactive UI (Streamlit)

| **MRR** | (1/Q) Ã— Î£(1/rank_i) | Measures retrieval quality |

| **Recall@10** | \|Relevant âˆ© Retrieved@10\| / \|Relevant\| | Coverage in top 10 |### âœ… Section 2: Evaluation Framework (10 pts)

| **BERTScore** | Semantic similarity | Generation quality |- [x] 100 Q&A pairs generated

- [x] MRR metric implemented

---- [x] Recall@10 metric implemented

- [x] Answer F1 metric implemented

## ğŸ“š Documentation- [x] Automated evaluation pipeline

- [x] HTML/CSV/JSON/PDF reports

| Document | Description |

|----------|-------------|### âœ… Submission Requirements

| [SUBMISSION_REFERENCE.md](SUBMISSION_REFERENCE.md) | Complete submission guide with all file links |- [x] Python source code (24 files)

| [docs/METRIC_JUSTIFICATION.md](docs/METRIC_JUSTIFICATION.md) | Metric selection rationale |- [x] PDF evaluation report

| [docs/NEW_FEATURES.md](docs/NEW_FEATURES.md) | New features documentation |- [x] Screenshots (3+)

| [submission/05_reports/Hybrid_RAG_Evaluation_Report.pdf](submission/05_reports/Hybrid_RAG_Evaluation_Report.pdf) | Full evaluation report |- [x] README documentation

- [x] 100-question dataset

---- [x] Evaluation results (300 rows)



## ğŸ”— Key Source Files---



| File | Purpose |## ğŸ“„ License

|------|---------|

| [chromadb_rag_system.py](chromadb_rag_system.py) | Core RAG implementation |This project is submitted as part of BITS Pilani Conversational AI coursework.

| [app_chromadb.py](app_chromadb.py) | Streamlit UI with chunk visualization |

| [evaluate_chromadb_fast.py](evaluate_chromadb_fast.py) | Evaluation pipeline |---

| [error_analysis.py](error_analysis.py) | Error analysis module |

**Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)

---

**Last Updated:** February 7, 2026

## ğŸ“‹ Requirements Checklist

### âœ… Part 1: Hybrid RAG System
- [x] Dense Vector Retrieval (ChromaDB + MiniLM)
- [x] Sparse Keyword Retrieval (BM25)
- [x] RRF Fusion (k=60)
- [x] Response Generation (FLAN-T5)
- [x] Interactive UI (Streamlit)
- [x] Chunk comparison visualization

### âœ… Part 2: Evaluation Framework
- [x] 100 Q&A pairs generated
- [x] MRR metric implemented
- [x] Recall@10 metric implemented
- [x] BERTScore metric implemented
- [x] Automated evaluation pipeline
- [x] PDF/CSV/JSON reports

---

## ğŸ“„ License

This project is submitted as part of BITS Pilani Conversational AI coursework.

---

**Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)
