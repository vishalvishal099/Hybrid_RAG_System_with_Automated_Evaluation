# ğŸ” Hybrid RAG System with Automated Evaluation# Hybrid RAG System with Automated Evaluation# Hybrid RAG System with Automated Evaluation# Hybrid RAG System with Automated Evaluation



[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue)](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)

[![Python](https://img.shields.io/badge/Python-3.10+-green)](https://python.org)

[![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-red)](https://streamlit.io)[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue)](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)



> A comprehensive **Hybrid RAG System** combining Dense Vector Retrieval, Sparse Keyword Retrieval, and Reciprocal Rank Fusion for Wikipedia-based Question Answering.



ğŸ“¦ **Repository:** [github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)A comprehensive implementation of a **Hybrid RAG System** combining **Dense Vector Retrieval (ChromaDB)**, **Sparse Keyword Retrieval (BM25)**, and **Reciprocal Rank Fusion (RRF)** to answer questions from Wikipedia articles.[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue)](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue)](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)



---



## ğŸ“‘ Table of Contents**GitHub Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)



- [Quick Start](#-quick-start)

- [Project Overview](#-project-overview)

- [System Architecture](#-system-architecture)**Last Updated:** February 8, 2026A comprehensive implementation of a **Hybrid RAG System** combining **Dense Vector Retrieval (ChromaDB)**, **Sparse Keyword Retrieval (BM25)**, and **Reciprocal Rank Fusion (RRF)** to answer questions from Wikipedia articles.A comprehensive implementation of a **Hybrid RAG System** combining **Dense Vector Retrieval (ChromaDB)**, **Sparse Keyword Retrieval (BM25)**, and **Reciprocal Rank Fusion (RRF)** to answer questions from Wikipedia articles.

- [Project Structure](#-project-structure)

- [Evaluation Results](#-evaluation-results)

- [Documentation](#-documentation)

- [Screenshots](#-screenshots)---

- [Contributors](#-contributors)



---

## ğŸš€ Quick Start**GitHub Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)**GitHub Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)

## ğŸš€ Quick Start



### Prerequisites

### Prerequisites

| Requirement | Version |

|-------------|---------|- Python 3.10+

| Python | 3.10+ |

| RAM | 4GB+ |- 4GB+ RAM**Last Updated:** February 8, 2026---

| Disk Space | 2GB+ |



### Installation

### Installation

```bash

# 1. Clone repository

git clone https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation.git

cd Hybrid_RAG_System_with_Automated_Evaluation```bash---## ğŸš€ Quick Start



# 2. Create virtual environment# Clone repository

python -m venv venv

source venv/bin/activate  # Windows: venv\Scripts\activategit clone https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation.git



# 3. Install dependenciescd Hybrid_RAG_System_with_Automated_Evaluation

pip install -r requirements.txt

```## ğŸš€ Quick Start### Prerequisites



### Run Application# Create virtual environment



```bashpython -m venv venv- Python 3.10+

# Start Streamlit Dashboard

streamlit run app_chromadb.py --server.port 8502source venv/bin/activate  # Windows: venv\Scripts\activate

```

### Prerequisites- 4GB+ RAM

ğŸŒ **Access URL:** http://localhost:8502

# Install dependencies

### Run Evaluation

pip install -r requirements.txt- Python 3.10+

```bash

# Run full evaluation (100 questions Ã— 3 methods)```

python evaluate_chromadb_fast.py

```- 4GB+ RAM### Installation



---### Run the Application



## ğŸ¯ Project Overview



This project implements a **state-of-the-art Hybrid RAG system** that:```bash



| Feature | Description |# Start Streamlit UI### Installation```bash

|---------|-------------|

| ğŸ”· **Dense Retrieval** | ChromaDB + MiniLM embeddings (384 dimensions) |streamlit run app_chromadb.py --server.port 8502

| ğŸ”¶ **Sparse Retrieval** | BM25 with NLTK tokenization |

| ğŸ”€ **Fusion** | Reciprocal Rank Fusion (RRF) with k=60 |```# Clone repository

| ğŸ¤– **Generation** | FLAN-T5-Base (248M parameters) |

| ğŸ“Š **Evaluation** | MRR, Recall@10, Answer F1 |

| ğŸ–¥ï¸ **Interface** | Interactive Streamlit Dashboard |

**Access URL:** http://localhost:8502```bashgit clone https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation.git

### ğŸ“ˆ Key Statistics



| Metric | Value |

|--------|-------|### Run Evaluation# Clone repositorycd Hybrid_RAG_System_with_Automated_Evaluation

| Wikipedia URLs | 500 (200 fixed + 300 random) |

| Total Chunks | 7,519 segments |

| Embedding Dimensions | 384 |

| Chunk Size | 500 characters |```bashgit clone https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation.git

| Evaluation Questions | 100 |

# Full evaluation (100 questions Ã— 3 methods)

---

python evaluate_chromadb_fast.pycd Hybrid_RAG_System_with_Automated_Evaluation# Create virtual environment

## ğŸ“Š System Architecture

```

```

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”python -m venv venv

                    â”‚     User Query      â”‚

                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜---

                               â”‚

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”# Create virtual environmentsource venv/bin/activate  # Windows: venv\Scripts\activate

                    â”‚  Query Processing   â”‚

                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜## ğŸ¯ Project Overview

                               â”‚

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”python -m venv venv

            â”‚                                     â”‚

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”This project implements a state-of-the-art Hybrid RAG system that:

  â”‚  Dense Retrieval  â”‚               â”‚  Sparse Retrieval   â”‚

  â”‚     (ChromaDB)    â”‚               â”‚       (BM25)        â”‚- Combines **dense** (ChromaDB + MiniLM) and **sparse** (BM25) retrievalsource venv/bin/activate  # Windows: venv\Scripts\activate# Install dependencies

  â”‚   + MiniLM-L6-v2  â”‚               â”‚      + NLTK         â”‚

  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- Uses **Reciprocal Rank Fusion (RRF)** with k=60 to merge results

            â”‚                                     â”‚

            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- Generates answers using **FLAN-T5** language model (248M parameters)pip install -r requirements.txt

                               â”‚

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- Includes comprehensive evaluation with **100 generated questions**

                    â”‚   RRF Fusion (k=60) â”‚

                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- Features automated evaluation pipeline with MRR, Recall@10, and Answer F1# Install dependencies```

                               â”‚

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- Interactive dashboard with **Dense/Sparse/Hybrid chunk comparison**

                    â”‚   Top-K Chunks      â”‚

                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜pip install -r requirements.txt

                               â”‚

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”### Key Statistics

                    â”‚  Answer Generation  â”‚

                    â”‚   (FLAN-T5-Base)    â”‚```### Run the Application

                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                               â”‚| Metric | Value |

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

                    â”‚  Generated Answer   â”‚|--------|-------|

                    â”‚   + Source URLs     â”‚

                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜| Total URLs | 500 (200 fixed + 300 random) |

```

| Total Chunks | 7,519 segments |### Run the Application```bash

---

| Embedding Dimensions | 384 |

## ğŸ—‚ï¸ Project Structure

| Chunk Size | 500 characters |# Start Streamlit UI

```

Hybrid_RAG_System_with_Automated_Evaluation/| Evaluation Questions | 100 |

â”‚

â”œâ”€â”€ ğŸ“„ Core Files```bash./start_ui.sh

â”‚   â”œâ”€â”€ chromadb_rag_system.py    # Core RAG implementation

â”‚   â”œâ”€â”€ app_chromadb.py           # Streamlit UI---

â”‚   â”œâ”€â”€ evaluate_chromadb_fast.py # Evaluation pipeline

â”‚   â”œâ”€â”€ error_analysis.py         # Error analysis# Start Streamlit UI

â”‚   â””â”€â”€ api_chromadb.py           # REST API

â”‚## ğŸ“Š System Architecture

â”œâ”€â”€ ğŸ“ data/

â”‚   â”œâ”€â”€ fixed_urls.json           # 200 Wikipedia URLsstreamlit run app_chromadb.py --server.port 8502# Or manually:

â”‚   â”œâ”€â”€ corpus.json               # 7,519 chunks

â”‚   â””â”€â”€ questions_100.json        # 100 Q&A pairs```

â”‚

â”œâ”€â”€ ğŸ“ evaluation/â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”```streamlit run app_chromadb.py

â”‚   â”œâ”€â”€ metrics.py                # MRR, Recall, F1

â”‚   â”œâ”€â”€ comprehensive_metrics.py  # Extended metricsâ”‚                      User Query                         â”‚

â”‚   â”œâ”€â”€ novel_metrics.py          # Custom metrics

â”‚   â”œâ”€â”€ pipeline.py               # Eval pipelineâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜```

â”‚   â””â”€â”€ run_evaluation.py         # Runner script

â”‚                           â”‚

â”œâ”€â”€ ğŸ“ src/

â”‚   â”œâ”€â”€ data_collection.py        # Wikipedia scraper              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”**Access URL:** http://localhost:8502

â”‚   â”œâ”€â”€ semantic_chunker.py       # Chunking logic

â”‚   â”œâ”€â”€ rrf_fusion.py             # RRF implementation              â”‚                         â”‚

â”‚   â””â”€â”€ question_generation.py    # Q&A generation

â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”### Run Evaluation

â”œâ”€â”€ ğŸ“ docs/

â”‚   â”œâ”€â”€ METRIC_JUSTIFICATION.md   # Metric rationale     â”‚  Dense Retrieval â”‚     â”‚ Sparse Retrieval â”‚

â”‚   â”œâ”€â”€ architecture_diagram.png  # System diagram

â”‚   â””â”€â”€ data_flow_diagram.png     # Data flow     â”‚    (ChromaDB +   â”‚     â”‚     (BM25 +      â”‚### Run Evaluation

â”‚

â”œâ”€â”€ ğŸ“ screenshots/               # UI screenshots     â”‚   MiniLM-L6-v2)  â”‚     â”‚      NLTK)       â”‚

â”œâ”€â”€ ğŸ“ submission/                # Submission package

â”œâ”€â”€ ğŸ“ chroma_db/                 # Vector database     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜```bash

â”‚

â”œâ”€â”€ ğŸ“„ requirements.txt           # Dependencies              â”‚                         â”‚

â”œâ”€â”€ ğŸ“„ config.yaml                # Configuration

â””â”€â”€ ğŸ“„ README.md                  # This file              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜```bash# Full evaluation (100 questions Ã— 3 methods)

```

                           â”‚

---

              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”# Full evaluation (100 questions Ã— 3 methods)python evaluate_chromadb_fast.py

## ğŸ“ˆ Evaluation Results

              â”‚   Reciprocal Rank       â”‚

### ğŸ† Performance Summary

              â”‚   Fusion (k=60)         â”‚python evaluate_chromadb_fast.py

| Method | MRR | Recall@10 | Avg Time |

|--------|-----|-----------|----------|              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

| Dense (ChromaDB) | 0.3025 | 0.33 | 5.86s |

| **Sparse (BM25)** | **0.4392** | **0.47** | **5.53s** |                           â”‚```# Generate reports

| Hybrid (RRF) | 0.3783 | 0.43 | 6.37s |

              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

> ğŸ’¡ **Key Finding:** BM25 outperforms Dense by **45%** on MRR for Wikipedia content.

              â”‚     Top-K Chunks        â”‚python generate_report.py

### ğŸ“Š Generation Metrics

              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

| Method | BLEU | ROUGE-L | BERTScore |

|--------|------|---------|-----------|                           â”‚---```

| Dense | 0.015 | 0.120 | 0.780 |

| Sparse | 0.022 | 0.145 | 0.820 |              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

| Hybrid | 0.018 | 0.135 | 0.810 |

              â”‚   Answer Generation     â”‚

### ğŸ“‹ Question Distribution

              â”‚    (FLAN-T5-base)       â”‚

| Type | Count | Percentage |

|------|-------|------------|              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜## ğŸ¯ Project Overview---

| Factual | 59 | 59% |

| Comparative | 15 | 15% |                           â”‚

| Inferential | 11 | 11% |

| Multi-hop | 15 | 15% |              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

| **Total** | **100** | 100% |

              â”‚    Generated Answer     â”‚

---

              â”‚    + Source URLs        â”‚This project implements a state-of-the-art Hybrid RAG system that:## ğŸ¯ Project Overview

## ğŸ“š Documentation

              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

| Document | Description | Link |

|----------|-------------|------|```- Combines **dense** (ChromaDB + MiniLM) and **sparse** (BM25) retrieval

| ğŸ“– Submission Guide | Complete submission reference | [SUBMISSION_REFERENCE_GUIDE.md](SUBMISSION_REFERENCE_GUIDE.md) |

| ğŸ“‹ Deliverables | Assignment mapping | [SUBMISSION_DELIVERABLES.md](SUBMISSION_DELIVERABLES.md) |

| ğŸ”— Quick Links | All GitHub links | [QUICK_ACCESS_LINKS.md](QUICK_ACCESS_LINKS.md) |

| ğŸ“Š Metrics | Metric justification | [docs/METRIC_JUSTIFICATION.md](docs/METRIC_JUSTIFICATION.md) |---- Uses **Reciprocal Rank Fusion (RRF)** with k=60 to merge resultsThis project implements a state-of-the-art Hybrid RAG system that:

| ğŸ“‘ Report | Full evaluation report | [submission/05_reports/Hybrid_RAG_Evaluation_Report.md](submission/05_reports/Hybrid_RAG_Evaluation_Report.md) |



---

## ğŸ—‚ï¸ Project Structure- Generates answers using **FLAN-T5** language model (248M parameters)- Combines **dense** (ChromaDB + MiniLM) and **sparse** (BM25) retrieval

## ğŸ“¸ Screenshots



### Query Interface

![Query Interface](screenshots/1_Hybrid_full_page.png)```- Includes comprehensive evaluation with **100 generated questions**- Uses **Reciprocal Rank Fusion (RRF)** with k=60 to merge results



### Dense Retrieval ModeHybrid_RAG_System_with_Automated_Evaluation/

![Dense Mode](screenshots/2_Dense_full_page.png)

â”‚- Features automated evaluation pipeline with MRR, Recall@10, and BERTScore- Generates answers using **FLAN-T5** language model

### Sparse Retrieval Mode

![Sparse Mode](screenshots/3_Sparse_full_page.png)â”œâ”€â”€ chromadb_rag_system.py      # Core RAG implementation



---â”œâ”€â”€ app_chromadb.py             # Streamlit UI with chunk visualization- Interactive dashboard with **Dense/Sparse/Hybrid chunk comparison**- Includes comprehensive evaluation with **100 generated questions**



## âœ… Requirements Checklistâ”œâ”€â”€ evaluate_chromadb_fast.py   # Evaluation pipeline



### Part 1: Hybrid RAG System (10 pts)â”œâ”€â”€ error_analysis.py           # Error analysis module- Features automated evaluation pipeline with MRR, Recall@10, and Answer F1

- [x] Dense Vector Retrieval (ChromaDB + MiniLM)

- [x] Sparse Keyword Retrieval (BM25)â”œâ”€â”€ api_chromadb.py             # REST API interface

- [x] RRF Fusion (k=60)

- [x] Response Generation (FLAN-T5)â”œâ”€â”€ build_chromadb_system.py    # System builder### Key Statistics

- [x] Interactive UI (Streamlit)

- [x] Chunk comparison visualizationâ”œâ”€â”€ setup.py                    # Package setup



### Part 2: Evaluation Framework (10 pts)â”œâ”€â”€ config.yaml                 # Configuration---

- [x] 100 Q&A pairs generated

- [x] MRR metric implementedâ”œâ”€â”€ requirements.txt            # Dependencies

- [x] Recall@10 metric implemented

- [x] Answer F1 metric implementedâ”œâ”€â”€ README.md                   # This file| Metric | Value |

- [x] Automated evaluation pipeline

- [x] PDF/CSV/JSON reportsâ”‚



---â”œâ”€â”€ data/|--------|-------|## ğŸ“Š System Architecture



## ğŸ‘¥ Contributorsâ”‚   â”œâ”€â”€ fixed_urls.json         # 200 fixed Wikipedia URLs



| Name | BITS ID |â”‚   â”œâ”€â”€ corpus.json             # Preprocessed corpus (7,519 chunks)| Total URLs | 500 (200 fixed + 300 random) |

|------|---------|

| VISHAL SINGH | 2024AA05641 |â”‚   â”œâ”€â”€ questions_100.json      # 100 evaluation questions

| GOBIND SAH | 2024AA05643 |

| YASH VERMA | 2024AA05640 |â”‚   â””â”€â”€ adversarial_questions.json  # 30 adversarial questions| Total Chunks | 7,519 segments |```

| AVISHI GUPTA | 2024AA05055 |

| SAYAN MANNA | 2024AB05304 |â”‚



---â”œâ”€â”€ chroma_db/                  # ChromaDB vector database| Embedding Dimensions | 384 |â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”



## ğŸ“„ Licenseâ”‚   â”œâ”€â”€ bm25_index.pkl          # BM25 index



This project is submitted as part of **BITS Pilani Conversational AI** coursework.â”‚   â”œâ”€â”€ bm25_corpus.pkl         # BM25 corpus| Chunk Size | 500 characters |â”‚                    User Query                            â”‚



---â”‚   â””â”€â”€ stats.json              # Database statistics



<p align="center">â”‚| Evaluation Questions | 100 |â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  <b>ğŸ“¦ Repository:</b> <a href="https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation">github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation</a>

  <br>â”œâ”€â”€ src/                        # Source modules

  <b>ğŸ“… Last Updated:</b> February 8, 2026

</p>â”‚   â”œâ”€â”€ data_collection.py      # Wikipedia data collector                       â”‚


â”‚   â”œâ”€â”€ semantic_chunker.py     # Semantic chunking

â”‚   â”œâ”€â”€ rrf_fusion.py           # RRF implementation---            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚   â”œâ”€â”€ rag_system.py           # RAG system

â”‚   â””â”€â”€ indexing.py             # Indexing utilities            â”‚                     â”‚

â”‚

â”œâ”€â”€ evaluation/                 # Evaluation framework## ğŸ“Š System Architecture    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”

â”‚   â”œâ”€â”€ metrics.py              # Core metrics (MRR, Recall)

â”‚   â”œâ”€â”€ novel_metrics.py        # Novel evaluation metrics    â”‚ Dense Retrievalâ”‚   â”‚Sparse Retrievalâ”‚

â”‚   â”œâ”€â”€ innovative_eval.py      # Innovative techniques

â”‚   â”œâ”€â”€ pipeline.py             # Evaluation pipeline```    â”‚ (ChromaDB +    â”‚   â”‚    (BM25 +     â”‚

â”‚   â””â”€â”€ comprehensive_metrics.py # Comprehensive metrics

â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  MiniLM-L6-v2) â”‚   â”‚     NLTK)      â”‚

â”œâ”€â”€ docs/                       # Documentation

â”‚   â”œâ”€â”€ METRIC_JUSTIFICATION.md # Metric selection rationaleâ”‚                    User Query                            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜

â”‚   â””â”€â”€ *.png                   # Visualizations

â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚                     â”‚

â”œâ”€â”€ screenshots/                # UI screenshots

â”‚   â”œâ”€â”€ 01_query_interface.png                       â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”‚   â”œâ”€â”€ 02_method_comparison.png

â”‚   â””â”€â”€ 03_evaluation_results.png            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚

â”‚

â”œâ”€â”€ submission/                 # Submission package            â”‚                     â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚   â”œâ”€â”€ 01_source_code/         # All source files

â”‚   â”œâ”€â”€ 02_data/                # Data files    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”            â”‚ Reciprocal Rank     â”‚

â”‚   â”œâ”€â”€ 03_vector_database/     # Database info

â”‚   â”œâ”€â”€ 04_evaluation_results/  # Results    â”‚ Dense Retrievalâ”‚   â”‚Sparse Retrievalâ”‚            â”‚ Fusion (k=60)       â”‚

â”‚   â”œâ”€â”€ 05_reports/             # PDF reports

â”‚   â”œâ”€â”€ 06_documentation/       # Docs    â”‚ (ChromaDB +    â”‚   â”‚    (BM25 +     â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”‚   â”œâ”€â”€ 07_visualizations/      # Charts

â”‚   â””â”€â”€ 08_screenshots/         # Screenshots    â”‚  MiniLM-L6-v2) â”‚   â”‚     NLTK)      â”‚                       â”‚

â”‚

â”œâ”€â”€ evaluation_results_chromadb.csv   # Evaluation results    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â””â”€â”€ evaluation_summary_chromadb.json  # Summary metrics

```            â”‚                     â”‚            â”‚   Top-K Chunks      â”‚



---            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



## ğŸ“ˆ Evaluation Results                       â”‚                       â”‚



### Performance Summary            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”



| Method | MRR | Recall@10 | Avg Time (s) |            â”‚ Reciprocal Rank     â”‚            â”‚  Answer Generation  â”‚

|--------|-----|-----------|--------------|

| Dense (ChromaDB) | 0.3025 | 0.33 | 5.86 |            â”‚ Fusion (k=60)       â”‚            â”‚  (FLAN-T5-base)     â”‚

| **Sparse (BM25)** | **0.4392** | **0.47** | 5.53 |

| Hybrid (RRF) | 0.3783 | 0.43 | 6.37 |            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



**Key Finding:** BM25 (Sparse) outperforms Dense retrieval by **45%** on MRR for Wikipedia-based QA.                       â”‚                       â”‚



### Generation Metrics            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”



| Method | BLEU | ROUGE-L | BERTScore |            â”‚   Top-K Chunks      â”‚            â”‚   Generated Answer  â”‚

|--------|------|---------|-----------|

| Dense | 0.015 | 0.120 | 0.780 |            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   + Source URLs     â”‚

| Sparse | 0.022 | 0.145 | 0.820 |

| Hybrid | 0.018 | 0.135 | 0.810 |                       â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



### Question Distribution            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”```



| Type | Count | Description |            â”‚  Answer Generation  â”‚

|------|-------|-------------|

| Factual | 59 | Direct fact-based questions |            â”‚  (FLAN-T5-base)     â”‚---

| Comparative | 15 | Questions comparing concepts |

| Inferential | 11 | Reasoning-based questions |            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

| Multi-hop | 15 | Questions requiring multiple sources |

| **Total** | **100** | - |                       â”‚## ğŸ—‚ï¸ Project Structure



---            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”



## ğŸ–¥ï¸ Interactive Dashboard Features            â”‚   Generated Answer  â”‚```



The Streamlit UI includes:            â”‚   + Source URLs     â”‚Hybrid_RAG_System_with_Automated_Evaluation/



| Feature | Description |            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚

|---------|-------------|

| **Query Input** | Text area for entering questions |```â”œâ”€â”€ chromadb_rag_system.py      # Core RAG implementation

| **Method Selection** | Choose Dense, Sparse, or Hybrid retrieval |

| **Chunk Score Visualization** | Interactive bar chart showing retrieval scores |â”œâ”€â”€ app_chromadb.py             # Streamlit UI (244 lines)

| **Dense Top 5 Chunks** | View top 5 chunks from ChromaDB |

| **Sparse Top 5 Chunks** | View top 5 chunks from BM25 |---â”œâ”€â”€ evaluate_chromadb_fast.py   # Evaluation pipeline

| **Hybrid Top 5 Chunks** | View top 5 chunks from RRF fusion |

| **Answer Display** | Generated answer with sources |â”œâ”€â”€ generate_report.py          # Report generator



---## ğŸ—‚ï¸ Project Structureâ”œâ”€â”€ start_ui.sh                 # Quick start script



## ğŸ› ï¸ Technical Detailsâ”‚



### Components```â”œâ”€â”€ data/



| Component | Technology | Details |Hybrid_RAG_System_with_Automated_Evaluation/â”‚   â”œâ”€â”€ fixed_urls.json         # 200 fixed Wikipedia URLs

|-----------|------------|---------|

| Dense Retrieval | ChromaDB + all-MiniLM-L6-v2 | 384-dim embeddings, 7,519 chunks |â”‚â”‚   â”œâ”€â”€ corpus.json             # Preprocessed corpus (14.5MB)

| Sparse Retrieval | BM25 + NLTK | Tokenization with rank_bm25 |

| Fusion | RRF | Reciprocal Rank Fusion with k=60 |â”œâ”€â”€ chromadb_rag_system.py      # Core RAG implementationâ”‚   â”œâ”€â”€ questions_100.json      # 100 evaluation questions

| Generation | FLAN-T5-base | 248M parameter model |

| UI | Streamlit | Interactive web interface |â”œâ”€â”€ app_chromadb.py             # Streamlit UI with chunk visualizationâ”‚   â””â”€â”€ indexes/                # BM25 index files



### Metricsâ”œâ”€â”€ evaluate_chromadb_fast.py   # Evaluation pipelineâ”‚



| Metric | Formula | Purpose |â”œâ”€â”€ error_analysis.py           # Error analysis moduleâ”œâ”€â”€ chroma_db/                  # ChromaDB vector database (212MB)

|--------|---------|---------|

| **MRR** | (1/Q) Ã— Î£(1/rank_i) | Measures retrieval quality |â”œâ”€â”€ api_chromadb.py             # REST API interfaceâ”‚

| **Recall@10** | \|Relevant âˆ© Retrieved@10\| / \|Relevant\| | Coverage in top 10 |

| **Answer F1** | 2Ã—(PÃ—R)/(P+R) | Token overlap with ground truth |â”œâ”€â”€ build_chromadb_system.py    # System builderâ”œâ”€â”€ docs/



---â”œâ”€â”€ setup.py                    # Package setupâ”‚   â”œâ”€â”€ METRIC_JUSTIFICATION.md # Metric selection rationale



## ğŸ“š Documentationâ”œâ”€â”€ config.yaml                 # Configurationâ”‚   â”œâ”€â”€ ERROR_ANALYSIS.md       # Failure analysis



| Document | Description |â”œâ”€â”€ requirements.txt            # Dependenciesâ”‚   â”œâ”€â”€ EVALUATION_REPORT.md    # Full evaluation report

|----------|-------------|

| [SUBMISSION_REFERENCE_GUIDE.md](SUBMISSION_REFERENCE_GUIDE.md) | Complete submission guide |â”œâ”€â”€ README.md                   # This fileâ”‚   â”œâ”€â”€ architecture_diagram.png

| [SUBMISSION_DELIVERABLES.md](SUBMISSION_DELIVERABLES.md) | Assignment requirement mapping |

| [QUICK_ACCESS_LINKS.md](QUICK_ACCESS_LINKS.md) | All GitHub links |â”œâ”€â”€ SUBMISSION_REFERENCE.md     # Complete submission guideâ”‚   â””â”€â”€ *.png                   # Visualizations

| [docs/METRIC_JUSTIFICATION.md](docs/METRIC_JUSTIFICATION.md) | Metric selection rationale |

| [submission/05_reports/Hybrid_RAG_Evaluation_Report.md](submission/05_reports/Hybrid_RAG_Evaluation_Report.md) | Full evaluation report |â”‚â”‚



---â”œâ”€â”€ data/â”œâ”€â”€ reports/



## ğŸ”— Key Source Filesâ”‚   â”œâ”€â”€ fixed_urls.json         # 200 fixed Wikipedia URLsâ”‚   â””â”€â”€ Hybrid_RAG_Evaluation_Report.pdf



| File | Purpose |â”‚   â”œâ”€â”€ corpus.json             # Preprocessed corpus (7,519 chunks)â”‚

|------|---------|

| [chromadb_rag_system.py](chromadb_rag_system.py) | Core RAG implementation |â”‚   â”œâ”€â”€ questions_100.json      # 100 evaluation questionsâ”œâ”€â”€ screenshots/

| [app_chromadb.py](app_chromadb.py) | Streamlit UI with chunk visualization |

| [evaluate_chromadb_fast.py](evaluate_chromadb_fast.py) | Evaluation pipeline |â”‚   â””â”€â”€ adversarial_questions.json  # 30 adversarial questionsâ”‚   â”œâ”€â”€ 01_query_interface.png

| [error_analysis.py](error_analysis.py) | Error analysis module |

â”‚â”‚   â”œâ”€â”€ 02_method_comparison.png

---

â”œâ”€â”€ chroma_db/                  # ChromaDB vector databaseâ”‚   â””â”€â”€ 03_evaluation_results.png

## ğŸ“¸ Screenshots

â”‚   â”œâ”€â”€ bm25_index.pkl          # BM25 indexâ”‚

### Query Interface

![Query Interface](screenshots/01_query_interface.png)â”‚   â”œâ”€â”€ bm25_corpus.pkl         # BM25 corpusâ”œâ”€â”€ evaluation_results_chromadb.csv     # 300 evaluation rows



### Method Comparison  â”‚   â””â”€â”€ stats.json              # Database statisticsâ”œâ”€â”€ evaluation_summary_chromadb.json    # Summary metrics

![Method Comparison](screenshots/02_method_comparison.png)

â”‚â”œâ”€â”€ evaluation_report_chromadb.html     # HTML report

### Evaluation Results

![Evaluation Results](screenshots/03_evaluation_results.png)â”œâ”€â”€ src/                        # Source modulesâ”‚



---â”‚   â”œâ”€â”€ data_collection.py      # Wikipedia data collectorâ””â”€â”€ README.md                   # This file



## ğŸ“‹ Requirements Checklistâ”‚   â”œâ”€â”€ semantic_chunker.py     # Semantic chunking```



### âœ… Part 1: Hybrid RAG System (10 pts)â”‚   â”œâ”€â”€ rrf_fusion.py           # RRF implementation

- [x] Dense Vector Retrieval (ChromaDB + MiniLM)

- [x] Sparse Keyword Retrieval (BM25)â”‚   â”œâ”€â”€ rag_system.py           # RAG system---

- [x] RRF Fusion (k=60)

- [x] Response Generation (FLAN-T5)â”‚   â””â”€â”€ indexing.py             # Indexing utilities

- [x] Interactive UI (Streamlit)

- [x] Chunk comparison visualizationâ”‚## ğŸ“ˆ Evaluation Results



### âœ… Part 2: Evaluation Framework (10 pts)â”œâ”€â”€ evaluation/                 # Evaluation framework

- [x] 100 Q&A pairs generated

- [x] MRR metric implementedâ”‚   â”œâ”€â”€ metrics.py              # Core metrics (MRR, BERTScore)### Performance Summary

- [x] Recall@10 metric implemented

- [x] Answer F1 metric implementedâ”‚   â”œâ”€â”€ novel_metrics.py        # Novel evaluation metrics

- [x] Automated evaluation pipeline

- [x] PDF/CSV/JSON reportsâ”‚   â”œâ”€â”€ innovative_eval.py      # Innovative techniques| Method | MRR | Recall@10 | Avg Time (s) | Questions |



---â”‚   â”œâ”€â”€ pipeline.py             # Evaluation pipeline|--------|-----|-----------|--------------|-----------|



## ğŸ‘¥ Contributorsâ”‚   â””â”€â”€ comprehensive_metrics.py # Comprehensive metrics| Dense (ChromaDB) | 0.3025 | 0.33 | 5.86 | 100 |



| Name | BITS ID |â”‚| **Sparse (BM25)** | **0.4392** | **0.47** | 5.53 | 100 |

|------|---------|

| VISHAL SINGH | 2024AA05641 |â”œâ”€â”€ docs/                       # Documentation| Hybrid (RRF) | 0.3783 | 0.43 | 6.37 | 100 |

| GOBIND SAH | 2024AA05643 |

| YASH VERMA | 2024AA05640 |â”‚   â”œâ”€â”€ METRIC_JUSTIFICATION.md # Metric selection rationale

| AVISHI GUPTA | 2024AA05055 |

| SAYAN MANNA | 2024AB05304 |â”‚   â”œâ”€â”€ NEW_FEATURES.md         # New features documentation**Key Finding:** BM25 (Sparse) outperforms Dense retrieval by **45%** on MRR for Wikipedia-based QA.



---â”‚   â”œâ”€â”€ architecture_diagram.png



## ğŸ“„ Licenseâ”‚   â”œâ”€â”€ data_flow_diagram.png### Question Distribution



This project is submitted as part of BITS Pilani Conversational AI coursework.â”‚   â””â”€â”€ retrieval_heatmap.png



---â”‚| Type | Count | Description |



**Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)â”œâ”€â”€ screenshots/                # UI screenshots|------|-------|-------------|


â”‚   â”œâ”€â”€ 01_query_interface.png| Factual | 59 | Direct fact-based questions |

â”‚   â”œâ”€â”€ 02_method_comparison.png| Comparative | 15 | Questions comparing concepts |

â”‚   â””â”€â”€ 03_evaluation_results.png| Inferential | 11 | Reasoning-based questions |

â”‚| Multi-hop | 15 | Questions requiring multiple sources |

â”œâ”€â”€ submission/                 # Submission package| **Total** | **100** | - |

â”‚   â”œâ”€â”€ 01_source_code/         # All source files

â”‚   â”œâ”€â”€ 02_data/                # Data files---

â”‚   â”œâ”€â”€ 03_vector_database/     # Database info

â”‚   â”œâ”€â”€ 04_evaluation_results/  # Results## ğŸ“š Documentation

â”‚   â”œâ”€â”€ 05_reports/             # PDF reports

â”‚   â”œâ”€â”€ 06_documentation/       # Docs| Document | Description | Link |

â”‚   â”œâ”€â”€ 07_visualizations/      # Charts|----------|-------------|------|

â”‚   â””â”€â”€ 08_screenshots/         # Screenshots| Metric Justification | Why MRR, Recall@10, Answer F1 | [docs/METRIC_JUSTIFICATION.md](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/docs/METRIC_JUSTIFICATION.md) |

â”‚| Error Analysis | Failure categorization | [docs/ERROR_ANALYSIS.md](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/docs/ERROR_ANALYSIS.md) |

â”œâ”€â”€ evaluation_results_chromadb.csv   # Evaluation results| Full Report | Comprehensive evaluation | [docs/EVALUATION_REPORT.md](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/docs/EVALUATION_REPORT.md) |

â””â”€â”€ evaluation_summary_chromadb.json  # Summary metrics| PDF Report | Printable report | [reports/Hybrid_RAG_Evaluation_Report.pdf](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/reports/Hybrid_RAG_Evaluation_Report.pdf) |

```

---

---

## ğŸ”— Key Source Files

## ğŸ“ˆ Evaluation Results

| File | Purpose | Link |

### Performance Summary|------|---------|------|

| `chromadb_rag_system.py` | Core RAG implementation | [View](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/chromadb_rag_system.py) |

| Method | MRR | Recall@10 | Avg Time (s) || `app_chromadb.py` | Streamlit UI | [View](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/app_chromadb.py) |

|--------|-----|-----------|--------------|| `evaluate_chromadb_fast.py` | Evaluation pipeline | [View](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/evaluate_chromadb_fast.py) |

| Dense (ChromaDB) | 0.3025 | 0.33 | 5.86 || `generate_report.py` | Report generation | [View](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation/blob/main/generate_report.py) |

| **Sparse (BM25)** | **0.4392** | **0.47** | 5.53 |

| Hybrid (RRF) | 0.3783 | 0.43 | 6.37 |---



**Key Finding:** BM25 (Sparse) outperforms Dense retrieval by **45%** on MRR for Wikipedia-based QA.## ğŸ“¸ Screenshots



### Generation Metrics### Query Interface

![Query Interface](screenshots/01_query_interface.png)

| Method | BLEU | ROUGE-L | BERTScore |

|--------|------|---------|-----------|### Method Comparison  

| Dense | 0.015 | 0.120 | 0.780 |![Method Comparison](screenshots/02_method_comparison.png)

| Sparse | 0.022 | 0.145 | 0.820 |

| Hybrid | 0.018 | 0.135 | 0.810 |### Evaluation Results

![Evaluation Results](screenshots/03_evaluation_results.png)

---

---

## ğŸ–¥ï¸ Interactive Dashboard Features

## ğŸ› ï¸ Technical Details

The Streamlit UI includes:

### Components

| Feature | Description |

|---------|-------------|| Component | Technology | Details |

| **Query Input** | Text area for entering questions ||-----------|------------|---------|

| **Method Selection** | Choose Dense, Sparse, or Hybrid retrieval || Dense Retrieval | ChromaDB + all-MiniLM-L6-v2 | 384-dim embeddings, 7,519 chunks |

| **Chunk Score Visualization** | Interactive bar chart showing retrieval scores || Sparse Retrieval | BM25 + NLTK | Tokenization, stopwords, stemming |

| **Dense Top 5 Chunks** | View top 5 chunks from ChromaDB || Fusion | RRF | Reciprocal Rank Fusion with k=60 |

| **Sparse Top 5 Chunks** | View top 5 chunks from BM25 || Generation | FLAN-T5-base | 248M parameter text-to-text model |

| **Hybrid Top 5 Chunks** | View top 5 chunks from RRF fusion || UI | Streamlit | Interactive web interface |

| **Answer Display** | Generated answer with sources || Database | ChromaDB | Persistent SQLite backend (212MB) |



---### Metrics



## ğŸ› ï¸ Technical Details| Metric | Formula | Purpose |

|--------|---------|---------|

### Components| **MRR** | (1/Q) Ã— Î£(1/rank_i) | Measures retrieval quality |

| **Recall@10** | \|Relevant âˆ© Retrieved@10\| / \|Relevant\| | Coverage in top 10 |

| Component | Technology | Details || **Answer F1** | 2Ã—(PÃ—R)/(P+R) | Token overlap with ground truth |

|-----------|------------|---------|

| Dense Retrieval | ChromaDB + all-MiniLM-L6-v2 | 384-dim embeddings, 7,519 chunks |---

| Sparse Retrieval | BM25 + NLTK | Tokenization with rank_bm25 |

| Fusion | RRF | Reciprocal Rank Fusion with k=60 |## ğŸ“‹ Requirements Checklist

| Generation | FLAN-T5-base | 248M parameter model |

| UI | Streamlit | Interactive web interface |### âœ… Section 1: Hybrid RAG System (10 pts)

- [x] Dense Vector Retrieval (ChromaDB + MiniLM)

### Metrics- [x] Sparse Keyword Retrieval (BM25)

- [x] RRF Fusion (k=60)

| Metric | Formula | Purpose |- [x] Response Generation (FLAN-T5)

|--------|---------|---------|- [x] Interactive UI (Streamlit)

| **MRR** | (1/Q) Ã— Î£(1/rank_i) | Measures retrieval quality |

| **Recall@10** | \|Relevant âˆ© Retrieved@10\| / \|Relevant\| | Coverage in top 10 |### âœ… Section 2: Evaluation Framework (10 pts)

| **BERTScore** | Semantic similarity | Generation quality |- [x] 100 Q&A pairs generated

- [x] MRR metric implemented

---- [x] Recall@10 metric implemented

- [x] Answer F1 metric implemented

## ğŸ“š Documentation- [x] Automated evaluation pipeline

- [x] HTML/CSV/JSON/PDF reports

| Document | Description |

|----------|-------------|### âœ… Submission Requirements

| [SUBMISSION_REFERENCE.md](SUBMISSION_REFERENCE.md) | Complete submission guide with all file links |- [x] Python source code (24 files)

| [docs/METRIC_JUSTIFICATION.md](docs/METRIC_JUSTIFICATION.md) | Metric selection rationale |- [x] PDF evaluation report

| [docs/NEW_FEATURES.md](docs/NEW_FEATURES.md) | New features documentation |- [x] Screenshots (3+)

| [submission/05_reports/Hybrid_RAG_Evaluation_Report.pdf](submission/05_reports/Hybrid_RAG_Evaluation_Report.pdf) | Full evaluation report |- [x] README documentation

- [x] 100-question dataset

---- [x] Evaluation results (300 rows)



## ğŸ”— Key Source Files---



| File | Purpose |## ğŸ“„ License

|------|---------|

| [chromadb_rag_system.py](chromadb_rag_system.py) | Core RAG implementation |This project is submitted as part of BITS Pilani Conversational AI coursework.

| [app_chromadb.py](app_chromadb.py) | Streamlit UI with chunk visualization |

| [evaluate_chromadb_fast.py](evaluate_chromadb_fast.py) | Evaluation pipeline |---

| [error_analysis.py](error_analysis.py) | Error analysis module |

**Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)

---

**Last Updated:** February 7, 2026

## ğŸ“‹ Requirements Checklist

### âœ… Part 1: Hybrid RAG System
- [x] Dense Vector Retrieval (ChromaDB + MiniLM)
- [x] Sparse Keyword Retrieval (BM25)
- [x] RRF Fusion (k=60)
- [x] Response Generation (FLAN-T5)
- [x] Interactive UI (Streamlit)
- [x] Chunk comparison visualization

### âœ… Part 2: Evaluation Framework
- [x] 100 Q&A pairs generated
- [x] MRR metric implemented
- [x] Recall@10 metric implemented
- [x] BERTScore metric implemented
- [x] Automated evaluation pipeline
- [x] PDF/CSV/JSON reports

---

## ğŸ“„ License

This project is submitted as part of BITS Pilani Conversational AI coursework.

---

**Repository:** [https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation](https://github.com/vishalvishal099/Hybrid_RAG_System_with_Automated_Evaluation)
