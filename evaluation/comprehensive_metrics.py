"""
Comprehensive Evaluation Metrics for RAG System
Implements: MRR, Recall@K, Answer F1, Ablation Tests, LLM-as-Judge
"""

import json
import re
from typing import List, Dict, Tuple, Set
from collections import Counter
import numpy as np


class EvaluationMetrics:
    """
    Comprehensive evaluation metrics for RAG systems
    """
    
    def __init__(self):
        self.results = []
    
    # ============ Retrieval Metrics ============
    
    def compute_mrr(
        self,
        retrieved_urls: List[str],
        ground_truth_url: str
    ) -> float:
        """
        Compute Mean Reciprocal Rank (MRR) for URL-based retrieval
        
        Args:
            retrieved_urls: List of retrieved document URLs (ordered by rank)
            ground_truth_url: The correct URL that should be retrieved
        
        Returns:
            Reciprocal rank (1/rank if found, 0 if not found)
        """
        for rank, url in enumerate(retrieved_urls, start=1):
            if url == ground_truth_url:
                return 1.0 / rank
        return 0.0
    
    def compute_recall_at_k(
        self,
        retrieved_urls: List[str],
        ground_truth_urls: List[str],
        k: int = 10
    ) -> float:
        """
        Compute Recall@K for URL-based retrieval
        
        Args:
            retrieved_urls: List of retrieved document URLs
            ground_truth_urls: List of all relevant URLs
            k: Cutoff for top-K results
        
        Returns:
            Recall@K score (proportion of relevant docs in top-K)
        """
        if not ground_truth_urls:
            return 0.0
        
        retrieved_set = set(retrieved_urls[:k])
        relevant_set = set(ground_truth_urls)
        
        intersection = retrieved_set & relevant_set
        recall = len(intersection) / len(relevant_set)
        
        return recall
    
    def compute_precision_at_k(
        self,
        retrieved_urls: List[str],
        ground_truth_urls: List[str],
        k: int = 10
    ) -> float:
        """
        Compute Precision@K for URL-based retrieval
        
        Args:
            retrieved_urls: List of retrieved document URLs
            ground_truth_urls: List of all relevant URLs
            k: Cutoff for top-K results
        
        Returns:
            Precision@K score
        """
        retrieved_set = set(retrieved_urls[:k])
        relevant_set = set(ground_truth_urls)
        
        intersection = retrieved_set & relevant_set
        precision = len(intersection) / k if k > 0 else 0.0
        
        return precision
    
    # ============ Answer Quality Metrics ============
    
    def tokenize_answer(self, text: str) -> List[str]:
        """Tokenize answer text for F1 computation"""
        # Lowercase and split on whitespace/punctuation
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)
        return tokens
    
    def compute_answer_f1(
        self,
        generated_answer: str,
        ground_truth_answer: str
    ) -> Dict[str, float]:
        """
        Compute token-level F1 score between generated and ground truth answers
        
        Args:
            generated_answer: Answer generated by RAG system
            ground_truth_answer: Reference/ground truth answer
        
        Returns:
            Dict with precision, recall, and f1 scores
        """
        generated_tokens = self.tokenize_answer(generated_answer)
        truth_tokens = self.tokenize_answer(ground_truth_answer)
        
        if not generated_tokens or not truth_tokens:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
        
        # Count token overlaps
        generated_counts = Counter(generated_tokens)
        truth_counts = Counter(truth_tokens)
        
        # True positives: tokens that appear in both
        tp = sum((generated_counts & truth_counts).values())
        
        # Precision: TP / (TP + FP) = TP / total_generated
        precision = tp / len(generated_tokens) if generated_tokens else 0.0
        
        # Recall: TP / (TP + FN) = TP / total_truth
        recall = tp / len(truth_tokens) if truth_tokens else 0.0
        
        # F1: harmonic mean of precision and recall
        if precision + recall > 0:
            f1 = 2 * (precision * recall) / (precision + recall)
        else:
            f1 = 0.0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
    
    def compute_exact_match(
        self,
        generated_answer: str,
        ground_truth_answer: str
    ) -> float:
        """
        Compute exact match score (1.0 if exact match, 0.0 otherwise)
        """
        gen_normalized = ' '.join(self.tokenize_answer(generated_answer))
        truth_normalized = ' '.join(self.tokenize_answer(ground_truth_answer))
        
        return 1.0 if gen_normalized == truth_normalized else 0.0
    
    # ============ Ablation Study Methods ============
    
    def compare_retrieval_methods(
        self,
        query: str,
        dense_only_results: List[Dict],
        sparse_only_results: List[Dict],
        hybrid_results: List[Dict],
        ground_truth_url: str
    ) -> Dict[str, float]:
        """
        Compare different retrieval methods (ablation study)
        
        Returns:
            Dict with MRR for each method
        """
        # Extract URLs from results
        dense_urls = [r.get('url') for r in dense_only_results if r.get('url')]
        sparse_urls = [r.get('url') for r in sparse_only_results if r.get('url')]
        hybrid_urls = [r.get('url') for r in hybrid_results if r.get('url')]
        
        return {
            'dense_only_mrr': self.compute_mrr(dense_urls, ground_truth_url),
            'sparse_only_mrr': self.compute_mrr(sparse_urls, ground_truth_url),
            'hybrid_mrr': self.compute_mrr(hybrid_urls, ground_truth_url)
        }
    
    # ============ LLM-as-Judge Methods ============
    
    def llm_judge_answer(
        self,
        question: str,
        generated_answer: str,
        reference_answer: str,
        judge_model=None
    ) -> Dict[str, float]:
        """
        Use LLM to judge answer quality on multiple dimensions
        
        Args:
            question: The question being answered
            generated_answer: Answer from RAG system
            reference_answer: Ground truth answer
            judge_model: LLM model to use for judging (FLAN-T5 or similar)
        
        Returns:
            Dict with scores for relevance, correctness, completeness
        """
        if judge_model is None:
            # Fallback to heuristic scoring
            return self._heuristic_judge(question, generated_answer, reference_answer)
        
        # Prepare prompts for each dimension
        prompts = {
            'relevance': f"""Question: {question}
Answer: {generated_answer}

On a scale of 0-10, how relevant is this answer to the question? Return only a number.
Score:""",
            
            'correctness': f"""Question: {question}
Answer: {generated_answer}
Reference: {reference_answer}

On a scale of 0-10, how correct is the answer compared to the reference? Return only a number.
Score:""",
            
            'completeness': f"""Question: {question}
Answer: {generated_answer}
Reference: {reference_answer}

On a scale of 0-10, how complete is the answer compared to the reference? Return only a number.
Score:"""
        }
        
        scores = {}
        for dimension, prompt in prompts.items():
            try:
                # Generate score using judge model
                response = judge_model.generate(prompt, max_length=10)
                # Extract numeric score
                score_match = re.search(r'\d+', response)
                if score_match:
                    scores[dimension] = float(score_match.group()) / 10.0
                else:
                    scores[dimension] = 0.5  # Default to mid-range
            except:
                scores[dimension] = 0.5
        
        # Compute overall score
        scores['overall'] = np.mean(list(scores.values()))
        
        return scores
    
    def _heuristic_judge(
        self,
        question: str,
        generated_answer: str,
        reference_answer: str
    ) -> Dict[str, float]:
        """
        Heuristic-based judging when LLM judge is unavailable
        """
        gen_tokens = set(self.tokenize_answer(generated_answer))
        ref_tokens = set(self.tokenize_answer(reference_answer))
        q_tokens = set(self.tokenize_answer(question))
        
        # Relevance: overlap with question terms
        relevance = len(gen_tokens & q_tokens) / len(q_tokens) if q_tokens else 0.0
        
        # Correctness: overlap with reference
        correctness = len(gen_tokens & ref_tokens) / len(ref_tokens) if ref_tokens else 0.0
        
        # Completeness: coverage of reference concepts
        completeness = len(gen_tokens & ref_tokens) / len(ref_tokens) if ref_tokens else 0.0
        
        # Overall
        overall = (relevance + correctness + completeness) / 3.0
        
        return {
            'relevance': min(relevance, 1.0),
            'correctness': min(correctness, 1.0),
            'completeness': min(completeness, 1.0),
            'overall': overall
        }
    
    # ============ Batch Evaluation ============
    
    def evaluate_system(
        self,
        test_cases: List[Dict],
        retrieval_fn,
        generation_fn,
        judge_model=None
    ) -> Dict:
        """
        Evaluate entire system on test cases
        
        Args:
            test_cases: List of dicts with {question, ground_truth_url, ground_truth_answer}
            retrieval_fn: Function that takes query and returns retrieved results
            generation_fn: Function that takes query and context to generate answer
            judge_model: Optional LLM judge model
        
        Returns:
            Dict with aggregated metrics
        """
        results = {
            'mrr_scores': [],
            'recall_at_10': [],
            'answer_f1': [],
            'llm_judge_scores': {
                'relevance': [],
                'correctness': [],
                'completeness': [],
                'overall': []
            }
        }
        
        for case in test_cases:
            query = case['question']
            gt_url = case.get('ground_truth_url')
            gt_answer = case.get('ground_truth_answer', '')
            gt_urls = case.get('ground_truth_urls', [gt_url] if gt_url else [])
            
            # Retrieval
            retrieved = retrieval_fn(query)
            retrieved_urls = [r.get('url') for r in retrieved if r.get('url')]
            
            # MRR
            if gt_url:
                mrr = self.compute_mrr(retrieved_urls, gt_url)
                results['mrr_scores'].append(mrr)
            
            # Recall@10
            if gt_urls:
                recall = self.compute_recall_at_k(retrieved_urls, gt_urls, k=10)
                results['recall_at_10'].append(recall)
            
            # Generation
            generated = generation_fn(query, retrieved[:5])  # Top-5 context
            
            # Answer F1
            if gt_answer:
                f1_scores = self.compute_answer_f1(generated, gt_answer)
                results['answer_f1'].append(f1_scores['f1'])
            
            # LLM Judge
            if gt_answer:
                judge_scores = self.llm_judge_answer(
                    query, generated, gt_answer, judge_model
                )
                for dim, score in judge_scores.items():
                    if dim in results['llm_judge_scores']:
                        results['llm_judge_scores'][dim].append(score)
        
        # Aggregate results
        aggregated = {
            'mean_mrr': np.mean(results['mrr_scores']) if results['mrr_scores'] else 0.0,
            'mean_recall_at_10': np.mean(results['recall_at_10']) if results['recall_at_10'] else 0.0,
            'mean_answer_f1': np.mean(results['answer_f1']) if results['answer_f1'] else 0.0,
            'llm_judge': {
                dim: np.mean(scores) if scores else 0.0
                for dim, scores in results['llm_judge_scores'].items()
            },
            'num_evaluated': len(test_cases)
        }
        
        return aggregated
